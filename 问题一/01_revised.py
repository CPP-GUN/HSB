# -*- coding: utf-8 -*-
"""
é—®é¢˜ä¸€ï¼šAIå‘å±•èƒ½åŠ›è¦ç´ è¯†åˆ«ä¸å…³è”åˆ†æ
=====================================
ç ”ç©¶ç›®æ ‡ï¼š
1. è¯†åˆ«èƒ½æœ‰æ•ˆè¯„ä¼°AIå‘å±•èƒ½åŠ›çš„è¦ç´ å¹¶é‡åŒ–
2. æ¢ç´¢è¦ç´ é—´çš„å†…åœ¨å…³è”ï¼ˆç›¸å…³æ€§åˆ†æï¼‰
3. åˆ†æè¦ç´ å¦‚ä½•ç›¸äº’ä½œç”¨ä¸å½±å“ï¼ˆPCA+å› æœåˆ†æï¼‰
4. æ­ç¤ºè¦ç´ å¦‚ä½•å…±åŒä¿ƒè¿›æˆ–åˆ¶çº¦AIå‘å±•
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering
from scipy import stats
from scipy.cluster.hierarchy import dendrogram, linkage
from matplotlib.font_manager import FontProperties
import matplotlib.patches as mpatches
from matplotlib.patches import FancyBboxPatch, FancyArrowPatch
import networkx as nx
import warnings
warnings.filterwarnings('ignore')

# è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
plt.rcParams['font.family'] = ['sans-serif']
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'KaiTi', 'Arial']
plt.rcParams['axes.unicode_minus'] = False

# åˆ›å»ºå­—ä½“å¯¹è±¡
FONT_CN = FontProperties(family='Microsoft YaHei', size=11)
FONT_TITLE = FontProperties(family='Microsoft YaHei', weight='bold', size=14)

print("="*70)
print("é—®é¢˜ä¸€ï¼šAIå‘å±•èƒ½åŠ›è¦ç´ è¯†åˆ«ä¸å…³è”åˆ†æ")
print("="*70)

# ==================== æ­¥éª¤1ï¼šçœŸå®æ•°æ®åŠ è½½ ====================

def generate_data():
    """
    æ­¥éª¤1ï¼šåŠ è½½çœŸå®æ•°æ® - 10ä¸ªå›½å®¶ Ã— 21ä¸ªè¦ç´ 
    æ•°æ®æ¥æºï¼šDATAæ–‡ä»¶å¤¹æ•´åˆçš„2023å¹´çœŸå®æ•°æ®
    """
    print("\nã€æ­¥éª¤1ã€‘åŠ è½½çœŸå®æ•°æ®")
    print("-" * 70)
    
    # è¯»å–æ•´åˆå¥½çš„çœŸå®æ•°æ®
    data = pd.read_csv('real_data_integrated.csv', encoding='utf-8-sig')
    
    # é‡å‘½ååˆ—ä¸ºä¸­æ–‡ç®€ç§°ï¼ˆä¿æŒåŸä»£ç å…¼å®¹ï¼‰
    column_mapping = {
        'T1_AIç ”ç©¶äººå‘˜æ•°é‡': 'AIç ”ç©¶äººå‘˜æ•°é‡',
        'T2_é¡¶å°–AIå­¦è€…æ•°é‡': 'é¡¶å°–AIå­¦è€…æ•°é‡',
        'T3_AIæ¯•ä¸šç”Ÿæ•°é‡': 'AIæ¯•ä¸šç”Ÿæ•°é‡',
        'A2_AIå¸‚åœºè§„æ¨¡': 'AIå¸‚åœºè§„æ¨¡',
        'A4_å¤§æ¨¡å‹æ•°é‡': 'å¤§æ¨¡å‹æ•°é‡',
        'P2_æ”¿ç­–æ•°é‡': 'AIæ”¿ç­–æ•°é‡',
        'P3_è¡¥è´´é‡‘é¢': 'AIè¡¥è´´é‡‘é¢',
        'R1_ä¼ä¸šç ”å‘æ”¯å‡º': 'ä¼ä¸šç ”å‘æ”¯å‡º',
        'R2_æ”¿åºœAIæŠ•èµ„': 'æ”¿åºœAIæŠ•èµ„',
        'R3_å›½é™…AIæŠ•èµ„': 'å›½é™…AIæŠ•èµ„',
        'I1_5Gè¦†ç›–ç‡': '5Gè¦†ç›–ç‡',
        'I2_GPUé›†ç¾¤è§„æ¨¡': 'GPUé›†ç¾¤è§„æ¨¡',
        'I3_äº’è”ç½‘å¸¦å®½': 'äº’è”ç½‘å¸¦å®½',
        'I4_äº’è”ç½‘æ™®åŠç‡': 'äº’è”ç½‘æ™®åŠç‡',
        'I5_ç”µèƒ½ç”Ÿäº§': 'ç”µèƒ½ç”Ÿäº§',
        'I6_AIç®—åŠ›å¹³å°æ•°é‡': 'AIç®—åŠ›å¹³å°',
        'I7_æ•°æ®ä¸­å¿ƒæ•°é‡': 'æ•°æ®ä¸­å¿ƒæ•°é‡',
        'I9_TOP500ä¸Šæ¦œæ•°': 'TOP500ä¸Šæ¦œæ•°',
        'O1_AI_Bookæ•°é‡': 'AI_Bookæ•°é‡',
        'O2_AI_Datasetæ•°é‡': 'AI_Datasetæ•°é‡',
        'O3_GitHubé¡¹ç›®æ•°': 'GitHubé¡¹ç›®æ•°'
    }
    
    data = data.rename(columns=column_mapping)
    
    print(f"  âœ“ æ•°æ®ç»´åº¦: {data.shape[0]} ä¸ªå›½å®¶ Ã— {data.shape[1]-1} ä¸ªè¦ç´ ")
    print(f"  âœ“ æ¶µç›–ç»´åº¦ï¼šT(äººæ‰)ã€A(åº”ç”¨)ã€P(æ”¿ç­–)ã€R(ç ”å‘)ã€I(åŸºç¡€è®¾æ–½)ã€O(äº§å‡º)")
    
    return data

def standardize_data(data):
    """
    å¯¹è¦ç´ è¿›è¡Œæ ‡å‡†åŒ–é‡åŒ–ï¼ˆMin-Maxå½’ä¸€åŒ–åˆ°[0,1]ï¼‰
    """
    print("\nã€æ­¥éª¤2ã€‘è¦ç´ æ ‡å‡†åŒ–é‡åŒ–")
    print("-" * 70)
    
    scaler = MinMaxScaler()
    countries = data['å›½å®¶']
    feature_names = [col for col in data.columns if col != 'å›½å®¶']
    X = data[feature_names].values
    
    X_scaled = scaler.fit_transform(X)
    
    standardized_df = pd.DataFrame(X_scaled, columns=feature_names)
    standardized_df.insert(0, 'å›½å®¶', countries)
    
    print(f"âœ“ æ‰€æœ‰è¦ç´ æ ‡å‡†åŒ–åˆ°[0, 1]åŒºé—´")
    print(f"âœ“ æ ‡å‡†åŒ–åæ•°æ®èŒƒå›´: [{X_scaled.min():.3f}, {X_scaled.max():.3f}]")
    
    return X_scaled, countries, feature_names, standardized_df

# ==================== æ­¥éª¤2ï¼šæ¢ç´¢è¦ç´ é—´å†…åœ¨å…³è” ====================

def correlation_analysis(X_scaled, feature_names):
    """
    æ­¥éª¤2ï¼šæ¢ç´¢è¦ç´ é—´çš„å†…åœ¨å…³è”
    ä½¿ç”¨Pearsonç›¸å…³ç³»æ•°åˆ†æ19ä¸ªè¦ç´ ä¹‹é—´çš„çº¿æ€§å…³ç³»
    """
    print("\nã€æ­¥éª¤3ã€‘æ¢ç´¢è¦ç´ é—´å†…åœ¨å…³è”ï¼ˆç›¸å…³æ€§åˆ†æï¼‰")
    print("-" * 70)
    
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = np.corrcoef(X_scaled.T)
    corr_df = pd.DataFrame(corr_matrix, index=feature_names, columns=feature_names)
    
    # æ‰¾å‡ºå¼ºç›¸å…³çš„è¦ç´ å¯¹ï¼ˆ|r| > 0.7ï¼‰
    strong_corr = []
    for i in range(len(feature_names)):
        for j in range(i+1, len(feature_names)):
            corr_val = corr_matrix[i, j]
            if abs(corr_val) > 0.7:
                strong_corr.append((feature_names[i], feature_names[j], corr_val))
    
    strong_corr.sort(key=lambda x: abs(x[2]), reverse=True)
    
    print(f"âœ“ å‘ç° {len(strong_corr)} å¯¹å¼ºç›¸å…³è¦ç´ ï¼ˆ|r| > 0.7ï¼‰")
    print("\nå¼ºç›¸å…³è¦ç´ å¯¹ï¼ˆå‰10ä¸ªï¼‰ï¼š")
    for i, (f1, f2, r) in enumerate(strong_corr[:10], 1):
        corr_type = "æ­£ç›¸å…³" if r > 0 else "è´Ÿç›¸å…³"
        print(f"  {i}. {f1} â†â†’ {f2}: r={r:.3f} ({corr_type})")
    
    return corr_df, strong_corr

def plot_correlation_heatmap(corr_df, feature_names):
    """ç»˜åˆ¶ç›¸å…³æ€§çƒ­åŠ›å›¾"""
    print("\nç»˜åˆ¶è¦ç´ ç›¸å…³æ€§çƒ­åŠ›å›¾...")
    
    plt.figure(figsize=(16, 14))
    
    mask = np.triu(np.ones_like(corr_df, dtype=bool), k=1)
    sns.heatmap(corr_df, annot=False, mask=mask, cmap='RdBu_r', center=0,
                square=True, linewidths=0.5, cbar_kws={"shrink": 0.8},
                fmt='.2f', vmin=-1, vmax=1)
    
    ax = plt.gca()
    ax.set_title('AIå‘å±•èƒ½åŠ›è¦ç´ ç›¸å…³æ€§çƒ­åŠ›å›¾\nï¼ˆæ¢ç´¢19ä¸ªè¦ç´ é—´çš„å†…åœ¨å…³è”ï¼‰', 
                 fontproperties=FONT_TITLE, pad=20)
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontproperties=FONT_CN)
    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontproperties=FONT_CN)
    
    plt.tight_layout()
    plt.savefig('fig1_è¦ç´ ç›¸å…³æ€§åˆ†æ.png', dpi=200, bbox_inches='tight')
    print("  âœ“ ä¿å­˜: fig1_è¦ç´ ç›¸å…³æ€§åˆ†æ.png")
    plt.close()

# ==================== æ­¥éª¤3ï¼šè¯†åˆ«å…³é”®è¦ç´ ï¼ˆPCAï¼‰ ====================

def pca_analysis(X_scaled, feature_names):
    """
    æ­¥éª¤3ï¼šä½¿ç”¨PCAè¯†åˆ«å…³é”®è¦ç´ å’Œè¦ç´ åˆ†ç»„
    """
    print("\nã€æ­¥éª¤4ã€‘è¯†åˆ«å…³é”®è¦ç´ ï¼ˆä¸»æˆåˆ†åˆ†æPCAï¼‰")
    print("-" * 70)
    
    # æ‰§è¡ŒPCA
    pca = PCA()
    X_pca = pca.fit_transform(X_scaled)
    
    explained_variance = pca.explained_variance_
    explained_variance_ratio = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(explained_variance_ratio)
    
    # ç¡®å®šä¸»æˆåˆ†æ•°é‡ï¼ˆç´¯ç§¯æ–¹å·®â‰¥85%ï¼‰
    n_components = np.where(cumulative_variance >= 0.85)[0][0] + 1
    
    print(f"âœ“ æå–äº† {n_components} ä¸ªä¸»æˆåˆ†ï¼ˆè§£é‡Š {cumulative_variance[n_components-1]*100:.2f}% çš„ä¿¡æ¯ï¼‰")
    print("\nå„ä¸»æˆåˆ†æ–¹å·®è´¡çŒ®ï¼š")
    for i in range(n_components):
        print(f"  ä¸»æˆåˆ†{i+1}: {explained_variance_ratio[i]*100:.2f}% (ç´¯ç§¯: {cumulative_variance[i]*100:.2f}%)")
    
    # è½½è·çŸ©é˜µï¼ˆæ˜¾ç¤ºæ¯ä¸ªè¦ç´ å¯¹ä¸»æˆåˆ†çš„è´¡çŒ®ï¼‰
    components = pca.components_
    loadings = pd.DataFrame(
        components[:n_components].T,
        index=feature_names,
        columns=[f'PC{i+1}' for i in range(n_components)]
    )
    
    # æ‰¾å‡ºæ¯ä¸ªä¸»æˆåˆ†çš„é«˜è½½è·è¦ç´ ï¼ˆ|loading| > 0.5ï¼‰
    print("\nå„ä¸»æˆåˆ†çš„å…³é”®è¦ç´ ï¼ˆ|è½½è·| > 0.5ï¼‰ï¼š")
    for i in range(n_components):
        pc_name = f'PC{i+1}'
        high_loadings = loadings[abs(loadings[pc_name]) > 0.5].sort_values(pc_name, key=abs, ascending=False)
        print(f"\n  {pc_name} ({explained_variance_ratio[i]*100:.1f}%):")
        for idx, val in high_loadings[pc_name].items():
            print(f"    {idx}: {val:.3f}")
    
    return pca, X_pca, explained_variance_ratio, cumulative_variance, loadings, n_components

def plot_pca_results(explained_variance_ratio, cumulative_variance, loadings, n_components):
    """ç»˜åˆ¶PCAåˆ†æç»“æœï¼ˆSCIé¡¶çº§æœŸåˆŠé£æ ¼ï¼‰"""
    print("\nç»˜åˆ¶PCAåˆ†æç»“æœ...")
    
    # ========== SCIçº§åˆ«è®¾ç½® ==========
    # ä½¿ç”¨Nature/Scienceæ¨èçš„é…è‰²æ–¹æ¡ˆ
    plt.style.use('seaborn-v0_8-paper')
    
    # åˆ›å»ºé«˜è´¨é‡ç”»å¸ƒ
    fig = plt.figure(figsize=(18, 7), dpi=300, facecolor='white')
    
    # Natureé…è‰²ï¼šä¸“ä¸šè“è‰²æ¸å˜
    colors_bar = ['#08519c', '#3182bd', '#6baed6', '#9ecae1', '#c6dbef', 
                  '#deebf7', '#f7fbff'][:n_components]
    
    # ========== å·¦å›¾ï¼šScree Plotï¼ˆç¢çŸ³å›¾ï¼‰==========
    ax1 = plt.subplot(1, 2, 1)
    x = np.arange(1, n_components + 1)
    
    # ä¸»æŸ±çŠ¶å›¾
    bars = ax1.bar(x, explained_variance_ratio[:n_components] * 100,
                   width=0.65, color=colors_bar, edgecolor='#2c3e50',
                   linewidth=2, alpha=0.85, zorder=3)
    
    # æŠ˜çº¿å›¾å åŠ ï¼ˆæ˜¾ç¤ºä¸‹é™è¶‹åŠ¿ï¼‰
    ax1.plot(x, explained_variance_ratio[:n_components] * 100,
            color='#e74c3c', linewidth=2.5, marker='D', markersize=8,
            markerfacecolor='white', markeredgewidth=2, markeredgecolor='#e74c3c',
            linestyle='--', alpha=0.8, zorder=4, label='Variance trend')
    
    # ç²¾ç¡®æ ‡æ³¨ï¼ˆåªæ ‡æ³¨å‰3ä¸ªä¸»æˆåˆ†ï¼‰
    for i in range(min(3, n_components)):
        height = bars[i].get_height()
        ax1.annotate(f'{explained_variance_ratio[i]*100:.1f}%',
                    xy=(bars[i].get_x() + bars[i].get_width()/2, height),
                    xytext=(0, 8), textcoords='offset points',
                    ha='center', va='bottom',
                    fontproperties=FONT_CN, fontsize=11, fontweight='bold',
                    bbox=dict(boxstyle='round,pad=0.4', facecolor='white', 
                             edgecolor='gray', alpha=0.8))
    
    # åæ ‡è½´æ ‡ç­¾ï¼ˆSCIè§„èŒƒï¼‰
    ax1.set_xlabel('Principal Component', fontproperties=FONT_CN, 
                   fontsize=14, fontweight='bold', labelpad=10)
    ax1.set_ylabel('Explained Variance (%)', fontproperties=FONT_CN,
                   fontsize=14, fontweight='bold', labelpad=10)
    ax1.set_title('(A) Scree Plot', fontproperties=FONT_TITLE,
                 fontsize=16, fontweight='bold', loc='left', pad=15)
    
    # ç½‘æ ¼ä¼˜åŒ–
    ax1.yaxis.grid(True, linestyle=':', alpha=0.4, linewidth=1, zorder=0)
    ax1.set_axisbelow(True)
    
    # è¾¹æ¡†ç¾åŒ–
    ax1.spines['top'].set_visible(False)
    ax1.spines['right'].set_visible(False)
    ax1.spines['left'].set_linewidth(1.5)
    ax1.spines['bottom'].set_linewidth(1.5)
    ax1.spines['left'].set_color('#2c3e50')
    ax1.spines['bottom'].set_color('#2c3e50')
    
    # åˆ»åº¦ä¼˜åŒ–
    ax1.tick_params(axis='both', which='major', labelsize=11, 
                   width=1.5, length=6, color='#2c3e50')
    ax1.set_ylim([0, max(explained_variance_ratio[:n_components]) * 110])
    ax1.set_xlim([0.3, n_components + 0.7])
    
    # å›¾ä¾‹
    ax1.legend(prop=FONT_CN, loc='upper right', fontsize=10,
              frameon=True, fancybox=True, shadow=True)
    
    # ========== å³å›¾ï¼šCumulative Varianceï¼ˆç´¯ç§¯æ–¹å·®ï¼‰==========
    ax2 = plt.subplot(1, 2, 2)
    
    # åŒYè½´è®¾è®¡ï¼ˆSCIå¸¸ç”¨æŠ€å·§ï¼‰
    cum_var = cumulative_variance[:n_components] * 100
    
    # æ¸å˜å¡«å……åŒºåŸŸï¼ˆä¸‰æ®µå¼ï¼‰
    for i in range(len(x)-1):
        alpha_val = 0.15 + (i / len(x)) * 0.25
        ax2.fill_between([x[i], x[i+1]], 0, [cum_var[i], cum_var[i+1]],
                        color='#3498db', alpha=alpha_val, zorder=1)
    
    # ä¸»æ›²çº¿ï¼ˆåŠ ç²—ä¸“ä¸šï¼‰
    ax2.plot(x, cum_var, color='#2980b9', linewidth=3.5,
            marker='o', markersize=11, markerfacecolor='white',
            markeredgewidth=2.5, markeredgecolor='#2980b9',
            label='Cumulative variance', zorder=3)
    
    # å…³é”®é˜ˆå€¼çº¿ï¼ˆ85%ï¼‰
    ax2.axhline(y=85, color='#e74c3c', linestyle='--', linewidth=2.5,
               alpha=0.9, zorder=2)
    ax2.axhline(y=90, color='#f39c12', linestyle=':', linewidth=2,
               alpha=0.7, zorder=2)
    
    # é˜ˆå€¼æ ‡æ³¨
    ax2.text(n_components * 0.98, 85, '85% threshold',
            fontproperties=FONT_CN, fontsize=10, color='#e74c3c',
            ha='right', va='bottom', fontweight='bold',
            bbox=dict(boxstyle='round,pad=0.5', facecolor='white',
                     edgecolor='#e74c3c', alpha=0.9))
    ax2.text(n_components * 0.98, 90, '90% threshold',
            fontproperties=FONT_CN, fontsize=9, color='#f39c12',
            ha='right', va='bottom',
            bbox=dict(boxstyle='round,pad=0.4', facecolor='white',
                     edgecolor='#f39c12', alpha=0.8))
    
    # ç²¾ç¡®æ•°å€¼æ ‡æ³¨ï¼ˆå‰3ä¸ªç‚¹ï¼‰
    for i in range(min(3, n_components)):
        ax2.scatter([x[i]], [cum_var[i]], s=150, c='#e74c3c',
                   edgecolors='white', linewidths=2, zorder=5, alpha=0.9)
        ax2.annotate(f'{cum_var[i]:.1f}%',
                    xy=(x[i], cum_var[i]), xytext=(10, 10),
                    textcoords='offset points', fontproperties=FONT_CN,
                    fontsize=10, fontweight='bold',
                    bbox=dict(boxstyle='round,pad=0.4', facecolor='#fff9e6',
                             edgecolor='#f39c12', linewidth=1.5))
    
    # åæ ‡è½´æ ‡ç­¾
    ax2.set_xlabel('Number of Components', fontproperties=FONT_CN,
                   fontsize=14, fontweight='bold', labelpad=10)
    ax2.set_ylabel('Cumulative Variance Explained (%)', fontproperties=FONT_CN,
                   fontsize=14, fontweight='bold', labelpad=10)
    ax2.set_title('(B) Cumulative Variance Plot', fontproperties=FONT_TITLE,
                 fontsize=16, fontweight='bold', loc='left', pad=15)
    
    # ç½‘æ ¼ä¼˜åŒ–
    ax2.yaxis.grid(True, linestyle=':', alpha=0.4, linewidth=1, zorder=0)
    ax2.xaxis.grid(True, linestyle=':', alpha=0.3, linewidth=0.8, zorder=0)
    ax2.set_axisbelow(True)
    
    # è¾¹æ¡†ç¾åŒ–
    ax2.spines['top'].set_visible(False)
    ax2.spines['right'].set_visible(False)
    ax2.spines['left'].set_linewidth(1.5)
    ax2.spines['bottom'].set_linewidth(1.5)
    ax2.spines['left'].set_color('#2c3e50')
    ax2.spines['bottom'].set_color('#2c3e50')
    
    # åˆ»åº¦ä¼˜åŒ–
    ax2.tick_params(axis='both', which='major', labelsize=11,
                   width=1.5, length=6, color='#2c3e50')
    ax2.set_ylim([0, 102])
    ax2.set_xlim([0.3, n_components + 0.7])
    
    # å›¾ä¾‹
    ax2.legend(prop=FONT_CN, loc='lower right', fontsize=10,
              frameon=True, fancybox=True, shadow=True)
    
    # ========== æ•´ä½“å¸ƒå±€ ==========
    plt.tight_layout(pad=3.0, w_pad=3.5)
    
    # SCIçº§åˆ«ä¿å­˜
    plt.savefig('fig2_PCAæ–¹å·®è§£é‡Š.png', dpi=300, bbox_inches='tight',
               facecolor='white', edgecolor='none', transparent=False)
    print("  âœ“ ä¿å­˜: fig2_PCAæ–¹å·®è§£é‡Š.pngï¼ˆSCIé¡¶çº§æœŸåˆŠé£æ ¼ï¼‰")
    plt.close()
    
    # æ¢å¤é»˜è®¤æ ·å¼
    plt.style.use('default')
    
    # å›¾2ï¼šè½½è·çŸ©é˜µçƒ­åŠ›å›¾
    plt.figure(figsize=(12, 10))
    
    sns.heatmap(loadings, annot=True, fmt='.2f', cmap='coolwarm', center=0,
                square=False, linewidths=0.5, cbar_kws={"shrink": 0.8},
                vmin=-1, vmax=1)
    
    ax = plt.gca()
    ax.set_title('ä¸»æˆåˆ†è½½è·çŸ©é˜µ\nï¼ˆæ­ç¤ºè¦ç´ å¦‚ä½•ç»„åˆå½¢æˆå…³é”®å› å­ï¼‰', 
                 fontproperties=FONT_TITLE, pad=20)
    ax.set_xlabel('ä¸»æˆåˆ†', fontproperties=FONT_CN)
    ax.set_ylabel('åŸå§‹è¦ç´ ', fontproperties=FONT_CN)
    ax.set_xticklabels(ax.get_xticklabels(), fontproperties=FONT_CN)
    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontproperties=FONT_CN)
    
    plt.tight_layout()
    plt.savefig('fig3_PCAè½½è·çŸ©é˜µ.png', dpi=200, bbox_inches='tight')
    print("  âœ“ ä¿å­˜: fig3_PCAè½½è·çŸ©é˜µ.png")
    plt.close()

# ==================== æ–°å¢ï¼šè¦ç´ èšç±»åˆ†æ ====================

def plot_factor_clustering(corr_df, feature_names):
    """
    ç»˜åˆ¶å±‚æ¬¡èšç±»çƒ­åŠ›å›¾ï¼ˆå±•ç¤ºè¦ç´ è‡ªç„¶åˆ†ç»„+ç›¸å…³æ€§ï¼‰
    """
    print("\nç»˜åˆ¶è¦ç´ å±‚æ¬¡èšç±»çƒ­åŠ›å›¾...")
    
    # æ¸…ç†ç›¸å…³ç³»æ•°çŸ©é˜µï¼ˆå¤„ç†å¯èƒ½çš„nan/infå€¼ï¼‰
    corr_matrix = corr_df.values.copy()
    corr_matrix = np.nan_to_num(corr_matrix, nan=0.0, posinf=1.0, neginf=-1.0)
    
    # ç¡®ä¿çŸ©é˜µå¯¹ç§°ä¸”å¯¹è§’çº¿ä¸º1
    corr_matrix = (corr_matrix + corr_matrix.T) / 2
    np.fill_diagonal(corr_matrix, 1.0)
    
    # é™åˆ¶å€¼åœ¨[-1, 1]èŒƒå›´å†…
    corr_matrix = np.clip(corr_matrix, -1, 1)
    
    # åˆ›å»ºæ–°çš„DataFrame
    clean_corr_df = pd.DataFrame(corr_matrix, index=feature_names, columns=feature_names)
    
    # ä½¿ç”¨seabornçš„clustermapè‡ªåŠ¨èšç±»å¹¶æ’åº
    plt.rcParams['font.family'] = ['Microsoft YaHei']
    
    # åˆ›å»ºclustermapï¼ˆä½¿ç”¨euclideanè·ç¦»é¿å…correlation metricçš„é—®é¢˜ï¼‰
    g = sns.clustermap(
        clean_corr_df, 
        cmap='RdBu_r',
        center=0,
        vmin=-1, 
        vmax=1,
        linewidths=0.5,
        figsize=(16, 14),
        dendrogram_ratio=0.15,
        cbar_pos=(0.02, 0.82, 0.03, 0.15),
        cbar_kws={
            'label': 'ç›¸å…³ç³»æ•°',
            'orientation': 'vertical'
        },
        method='average',
        metric='euclidean',  # ä½¿ç”¨euclideanè·ç¦»æ›´ç¨³å®š
        row_cluster=True,
        col_cluster=True,
        xticklabels=True,
        yticklabels=True
    )
    
    # è®¾ç½®æ ‡é¢˜
    g.fig.suptitle('AIå‘å±•è¦ç´ å±‚æ¬¡èšç±»çƒ­åŠ›å›¾\nï¼ˆè‡ªåŠ¨åˆ†ç»„+ç›¸å…³æ€§åˆ†æï¼‰', 
                   fontproperties=FONT_TITLE, y=0.98, fontsize=16)
    
    # è®¾ç½®åˆ»åº¦æ ‡ç­¾å­—ä½“
    ax = g.ax_heatmap
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', 
                       fontproperties=FONT_CN, fontsize=10)
    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, 
                       fontproperties=FONT_CN, fontsize=10)
    
    # è®¾ç½®colorbaræ ‡ç­¾å­—ä½“
    cbar = g.ax_cbar
    cbar.set_ylabel('ç›¸å…³ç³»æ•°', fontproperties=FONT_CN, fontsize=11)
    
    # æ·»åŠ è¯´æ˜æ–‡å­—
    g.fig.text(0.02, 0.02, 
              'æ³¨ï¼šç›¸ä¼¼è¦ç´ è‡ªåŠ¨èšåœ¨ä¸€èµ·ï½œçº¢è‰²=æ­£ç›¸å…³ï½œè“è‰²=è´Ÿç›¸å…³ï½œæ ‘çŠ¶å›¾æ˜¾ç¤ºèšç±»ç»“æ„',
              fontproperties=FONT_CN, fontsize=9, style='italic', color='gray')
    
    plt.savefig('fig5_è¦ç´ èšç±»çƒ­åŠ›å›¾.png', dpi=200, bbox_inches='tight')
    print("  âœ“ ä¿å­˜: fig5_è¦ç´ èšç±»çƒ­åŠ›å›¾.png")
    plt.close()
    
    # æå–èšç±»åçš„é¡ºåº
    row_order = g.dendrogram_row.reordered_ind
    reordered_features = [feature_names[i] for i in row_order]
    
    print(f"  âœ“ èšç±»åçš„è¦ç´ é¡ºåºï¼ˆç›¸ä¼¼è¦ç´ ç›¸é‚»ï¼‰ï¼š")
    for i, feat in enumerate(reordered_features, 1):
        print(f"     {i:2d}. {feat}")
    
    return reordered_features

def plot_factor_importance(loadings, explained_variance_ratio):
    """
    ç»˜åˆ¶è¦ç´ é‡è¦æ€§æ’åå›¾ï¼ˆåŸºäºPCAè´¡çŒ®åº¦ï¼‰
    """
    print("\nç»˜åˆ¶è¦ç´ é‡è¦æ€§æ’åå›¾...")
    
    # è®¡ç®—æ¯ä¸ªè¦ç´ çš„ç»¼åˆé‡è¦æ€§ï¼ˆåŠ æƒè½½è·å¹³æ–¹å’Œï¼‰
    n_components = min(len(explained_variance_ratio), loadings.shape[1])
    importance_scores = np.zeros(len(loadings))
    
    for i in range(n_components):
        importance_scores += (loadings.iloc[:, i].values ** 2) * explained_variance_ratio[i]
    
    importance_df = pd.DataFrame({
        'è¦ç´ ': loadings.index,
        'é‡è¦æ€§å¾—åˆ†': importance_scores
    }).sort_values('é‡è¦æ€§å¾—åˆ†', ascending=True)
    
    # ç»˜åˆ¶æ¨ªå‘æŸ±çŠ¶å›¾
    fig, ax = plt.subplots(figsize=(12, 10))
    
    colors = plt.cm.RdYlGn(importance_df['é‡è¦æ€§å¾—åˆ†'] / importance_df['é‡è¦æ€§å¾—åˆ†'].max())
    bars = ax.barh(importance_df['è¦ç´ '], importance_df['é‡è¦æ€§å¾—åˆ†'], color=colors)
    
    ax.set_xlabel('é‡è¦æ€§å¾—åˆ†', fontproperties=FONT_CN)
    ax.set_title('AIå‘å±•è¦ç´ é‡è¦æ€§æ’å\nï¼ˆåŸºäºPCAæ–¹å·®è´¡çŒ®åº¦ï¼‰', fontproperties=FONT_TITLE, pad=15)
    
    # æ ‡æ³¨åˆ†æ•°
    for i, (factor, score) in enumerate(zip(importance_df['è¦ç´ '], importance_df['é‡è¦æ€§å¾—åˆ†'])):
        ax.text(score + 0.001, i, f'{score:.3f}', va='center', fontproperties=FONT_CN, fontsize=9)
    
    ax.set_yticklabels(importance_df['è¦ç´ '], fontproperties=FONT_CN, fontsize=10)
    ax.grid(axis='x', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('fig6_è¦ç´ é‡è¦æ€§æ’å.png', dpi=200, bbox_inches='tight')
    print("  âœ“ ä¿å­˜: fig6_è¦ç´ é‡è¦æ€§æ’å.png")
    plt.close()
    
    return importance_df

def plot_causal_network(corr_df, feature_names):
    """
    ç»˜åˆ¶è¦ç´ å…³ç³»ç½‘ç»œå›¾ï¼ˆå±•ç¤ºå¼ºç›¸å…³å…³ç³»ï¼‰
    """
    print("\nç»˜åˆ¶è¦ç´ å…³ç³»ç½‘ç»œå›¾...")
    
    # åˆ›å»ºç½‘ç»œå›¾
    G = nx.Graph()
    
    # è¦ç´ åˆ†ç±»ï¼ˆç”¨äºç€è‰²ï¼‰
    factor_categories = {
        'åŸºç¡€è®¾æ–½': ['AIç®—åŠ›è§„æ¨¡', 'äº‘è®¡ç®—èƒ½åŠ›', '5G/6Gè¦†ç›–ç‡'],
        'äººæ‰å‚¨å¤‡': ['AIç ”ç©¶äººå‘˜æ•°é‡', 'é¡¶å°–AIå­¦è€…æ•°é‡', 'AIæ¯•ä¸šç”Ÿæ•°é‡'],
        'ç ”å‘æŠ•å…¥': ['æ”¿åºœAIç ”å‘ç»è´¹', 'ä¼ä¸šAIæŠ•èµ„é¢', 'ç ”å‘å¼ºåº¦', 'å¤§å‹AIå®éªŒå®¤æ•°'],
        'äº§ä¸šåº”ç”¨': ['AIä¼ä¸šæ•°é‡', 'AIå¸‚åœºè§„æ¨¡', 'AIåº”ç”¨æ¸—é€ç‡'],
        'æ”¿ç­–ç¯å¢ƒ': ['AIå›½å®¶æˆ˜ç•¥', 'æ•°æ®å¼€æ”¾ç¨‹åº¦', 'çŸ¥è¯†äº§æƒä¿æŠ¤'],
        'åˆ›æ–°äº§å‡º': ['AIé¡¶ä¼šè®ºæ–‡æ•°', 'AIä¸“åˆ©ç”³è¯·é‡', 'GitHubå¼€æºè´¡çŒ®']
    }
    
    # ä¸ºæ¯ä¸ªè¦ç´ åˆ†é…ç±»åˆ«
    factor_to_category = {}
    category_colors = {
        'åŸºç¡€è®¾æ–½': '#FF6B6B',
        'äººæ‰å‚¨å¤‡': '#4ECDC4',
        'ç ”å‘æŠ•å…¥': '#45B7D1',
        'äº§ä¸šåº”ç”¨': '#FFA07A',
        'æ”¿ç­–ç¯å¢ƒ': '#98D8C8',
        'åˆ›æ–°äº§å‡º': '#FFD93D'
    }
    
    for category, factors in factor_categories.items():
        for factor in factors:
            factor_to_category[factor] = category
    
    # æ·»åŠ èŠ‚ç‚¹
    for factor in feature_names:
        G.add_node(factor, category=factor_to_category.get(factor, 'å…¶ä»–'))
    
    # æ·»åŠ å¼ºç›¸å…³è¾¹ï¼ˆ|r| > 0.7ï¼‰
    for i in range(len(feature_names)):
        for j in range(i+1, len(feature_names)):
            corr_val = corr_df.iloc[i, j]
            if abs(corr_val) > 0.7:
                G.add_edge(feature_names[i], feature_names[j], weight=abs(corr_val))
    
    # ç»˜å›¾
    fig, ax = plt.subplots(figsize=(18, 16))
    
    # ä½¿ç”¨springå¸ƒå±€
    pos = nx.spring_layout(G, k=2, iterations=50, seed=42)
    
    # ç»˜åˆ¶ç¤¾åŒºè¾¹æ¡†ï¼ˆåœˆå‡ºåŒç±»åˆ«è¦ç´ ï¼‰
    for category, color in category_colors.items():
        nodes = [n for n, attr in G.nodes(data=True) if attr.get('category') == category]
        if len(nodes) > 0:
            # è·å–è¯¥ç±»åˆ«æ‰€æœ‰èŠ‚ç‚¹çš„åæ ‡
            node_positions = np.array([pos[n] for n in nodes])
            x_coords = node_positions[:, 0]
            y_coords = node_positions[:, 1]
            
            # è®¡ç®—å‡¸åŒ…ï¼ˆå¤–å›´è½®å»“ï¼‰
            if len(nodes) >= 3:
                from scipy.spatial import ConvexHull
                try:
                    hull = ConvexHull(node_positions)
                    # ç»˜åˆ¶å‡¸åŒ…å¤šè¾¹å½¢ï¼ˆåŠé€æ˜èƒŒæ™¯æ¡†ï¼‰
                    for simplex in hull.simplices:
                        ax.fill(node_positions[simplex, 0], node_positions[simplex, 1], 
                               color=color, alpha=0.15, zorder=0)
                    # ç»˜åˆ¶å‡¸åŒ…è¾¹ç•Œï¼ˆå®çº¿æ¡†ï¼‰
                    hull_points = node_positions[hull.vertices]
                    hull_points = np.vstack([hull_points, hull_points[0]])  # é—­åˆ
                    ax.plot(hull_points[:, 0], hull_points[:, 1], 
                           color=color, linewidth=3, alpha=0.6, linestyle='--', zorder=1)
                except:
                    # å¦‚æœå‡¸åŒ…è®¡ç®—å¤±è´¥ï¼Œç”»ä¸€ä¸ªåœ†å½¢åŒºåŸŸ
                    center_x, center_y = np.mean(x_coords), np.mean(y_coords)
                    radius = np.max(np.sqrt((x_coords - center_x)**2 + (y_coords - center_y)**2)) + 0.15
                    circle = plt.Circle((center_x, center_y), radius, color=color, 
                                       alpha=0.15, fill=True, zorder=0)
                    ax.add_patch(circle)
                    circle_edge = plt.Circle((center_x, center_y), radius, color=color, 
                                            alpha=0.6, fill=False, linewidth=3, 
                                            linestyle='--', zorder=1)
                    ax.add_patch(circle_edge)
            else:
                # å°‘äº3ä¸ªèŠ‚ç‚¹ï¼Œç”»åœ†
                center_x, center_y = np.mean(x_coords), np.mean(y_coords)
                radius = 0.15 if len(nodes) == 1 else np.max(np.sqrt((x_coords - center_x)**2 + (y_coords - center_y)**2)) + 0.1
                circle = plt.Circle((center_x, center_y), radius, color=color, 
                                   alpha=0.15, fill=True, zorder=0)
                ax.add_patch(circle)
                circle_edge = plt.Circle((center_x, center_y), radius, color=color, 
                                        alpha=0.6, fill=False, linewidth=3, 
                                        linestyle='--', zorder=1)
                ax.add_patch(circle_edge)
    
    # ç»˜åˆ¶è¾¹ï¼ˆç²—ç»†è¡¨ç¤ºç›¸å…³å¼ºåº¦ï¼‰
    edges = G.edges()
    weights = [G[u][v]['weight'] for u, v in edges]
    nx.draw_networkx_edges(G, pos, width=[w*3 for w in weights], 
                          alpha=0.5, edge_color='#34495e', ax=ax)
    
    # ç»˜åˆ¶èŠ‚ç‚¹ï¼ˆæŒ‰ç±»åˆ«ç€è‰²ï¼‰
    for category, color in category_colors.items():
        nodes = [n for n, attr in G.nodes(data=True) if attr.get('category') == category]
        nx.draw_networkx_nodes(G, pos, nodelist=nodes, node_color=color, 
                              node_size=1000, alpha=0.95, ax=ax, label=category,
                              edgecolors='white', linewidths=2.5)
    
    # ç»˜åˆ¶æ ‡ç­¾
    labels = {node: node for node in G.nodes()}
    for node, (x, y) in pos.items():
        ax.text(x, y, node, fontproperties=FONT_CN, fontsize=9, 
               fontweight='bold', ha='center', va='center',
               bbox=dict(boxstyle='round,pad=0.4', facecolor='white', 
                        alpha=0.9, edgecolor='gray', linewidth=1.5))
    
    ax.set_title('AIå‘å±•è¦ç´ å…³ç³»ç½‘ç»œå›¾\nï¼ˆèŠ‚ç‚¹=è¦ç´ ï¼Œè™šçº¿åœˆ=ç±»åˆ«ï¼Œè¿çº¿=å¼ºç›¸å…³|r|>0.7ï¼‰', 
                fontproperties=FONT_TITLE, pad=20, fontsize=15)
    ax.legend(prop=FONT_CN, loc='upper left', fontsize=11, framealpha=0.95,
             title='è¦ç´ ç±»åˆ«', title_fontproperties=FONT_CN)
    ax.axis('off')
    
    plt.tight_layout()
    plt.savefig('fig7_è¦ç´ å…³ç³»ç½‘ç»œ.png', dpi=200, bbox_inches='tight')
    print("  âœ“ ä¿å­˜: fig7_è¦ç´ å…³ç³»ç½‘ç»œ.png")
    plt.close()

def plot_causal_paths(feature_names):
    """
    ç»˜åˆ¶å…³é”®è¦ç´ å½±å“è·¯å¾„å›¾ï¼ˆç®­å¤´æµç¨‹å›¾ï¼‰
    """
    print("\nç»˜åˆ¶è¦ç´ å½±å“è·¯å¾„å›¾...")
    
    fig, ax = plt.subplots(figsize=(16, 12))
    ax.set_xlim(0, 10)
    ax.set_ylim(0, 10)
    ax.axis('off')
    
    # æ ‡é¢˜
    ax.text(5, 9.5, 'AIå‘å±•è¦ç´ ç›¸äº’ä½œç”¨æœºåˆ¶å›¾', 
           fontproperties=FONT_TITLE, fontsize=18, ha='center', weight='bold')
    
    # å®šä¹‰è¦ç´ ç»„çš„ä½ç½®
    positions = {
        'åŸºç¡€è®¾æ–½': (1.5, 7),
        'æ”¿ç­–ç¯å¢ƒ': (1.5, 4.5),
        'ç ”å‘æŠ•å…¥': (4.5, 5.5),
        'äººæ‰å‚¨å¤‡': (7.5, 7),
        'äº§ä¸šåº”ç”¨': (7.5, 4.5),
        'åˆ›æ–°äº§å‡º': (7.5, 2)
    }
    
    colors = {
        'åŸºç¡€è®¾æ–½': '#FF6B6B',
        'æ”¿ç­–ç¯å¢ƒ': '#98D8C8',
        'ç ”å‘æŠ•å…¥': '#45B7D1',
        'äººæ‰å‚¨å¤‡': '#4ECDC4',
        'äº§ä¸šåº”ç”¨': '#FFA07A',
        'åˆ›æ–°äº§å‡º': '#FFD93D'
    }
    
    # ç»˜åˆ¶è¦ç´ ç»„æ¡†
    for group, (x, y) in positions.items():
        box = FancyBboxPatch((x-0.6, y-0.35), 1.2, 0.7,
                            boxstyle="round,pad=0.1",
                            facecolor=colors[group], 
                            edgecolor='black', 
                            linewidth=2, 
                            alpha=0.8)
        ax.add_patch(box)
        ax.text(x, y, group, fontproperties=FONT_CN, fontsize=12,
               ha='center', va='center', weight='bold')
    
    # å®šä¹‰å½±å“å…³ç³»ï¼ˆèµ·ç‚¹, ç»ˆç‚¹, å½±å“å¼ºåº¦, å…³ç³»ç±»å‹ï¼‰
    relationships = [
        ('ç ”å‘æŠ•å…¥', 'äººæ‰å‚¨å¤‡', 0.69, 'å¼ºä¿ƒè¿›'),
        ('ç ”å‘æŠ•å…¥', 'åˆ›æ–°äº§å‡º', 0.54, 'ä¸­ç­‰ä¿ƒè¿›'),
        ('ç ”å‘æŠ•å…¥', 'äº§ä¸šåº”ç”¨', 0.71, 'å¼ºä¿ƒè¿›'),
        ('äººæ‰å‚¨å¤‡', 'åˆ›æ–°äº§å‡º', 0.79, 'æå¼ºä¿ƒè¿›'),
        ('åŸºç¡€è®¾æ–½', 'äº§ä¸šåº”ç”¨', 0.49, 'æ”¯æ’‘'),
        ('æ”¿ç­–ç¯å¢ƒ', 'ç ”å‘æŠ•å…¥', 0.12, 'å¼±é—´æ¥'),
        ('æ”¿ç­–ç¯å¢ƒ', 'äººæ‰å‚¨å¤‡', -0.13, 'å¼±å…³è”'),
        ('äº§ä¸šåº”ç”¨', 'åˆ›æ–°äº§å‡º', 0.45, 'ä¿ƒè¿›')
    ]
    
    # ç»˜åˆ¶ç®­å¤´
    for start, end, strength, rel_type in relationships:
        x1, y1 = positions[start]
        x2, y2 = positions[end]
        
        # æ ¹æ®å¼ºåº¦è®¾ç½®ç®­å¤´æ ·å¼
        if abs(strength) > 0.7:
            linewidth = 4
            alpha = 0.9
            color = 'darkgreen'
        elif abs(strength) > 0.5:
            linewidth = 3
            alpha = 0.7
            color = 'green'
        elif abs(strength) > 0.3:
            linewidth = 2
            alpha = 0.5
            color = 'orange'
        else:
            linewidth = 1.5
            alpha = 0.4
            color = 'gray'
        
        # è®¡ç®—ç®­å¤´èµ·ç‚¹å’Œç»ˆç‚¹ï¼ˆé¿å¼€æ–¹æ¡†ï¼‰
        dx = x2 - x1
        dy = y2 - y1
        dist = np.sqrt(dx**2 + dy**2)
        dx_norm = dx / dist
        dy_norm = dy / dist
        
        arrow_start_x = x1 + dx_norm * 0.7
        arrow_start_y = y1 + dy_norm * 0.4
        arrow_end_x = x2 - dx_norm * 0.7
        arrow_end_y = y2 - dy_norm * 0.4
        
        arrow = FancyArrowPatch(
            (arrow_start_x, arrow_start_y),
            (arrow_end_x, arrow_end_y),
            arrowstyle='->,head_width=0.4,head_length=0.4',
            linewidth=linewidth,
            color=color,
            alpha=alpha,
            connectionstyle="arc3,rad=0.1"
        )
        ax.add_patch(arrow)
        
        # æ ‡æ³¨ç›¸å…³ç³»æ•°
        mid_x = (arrow_start_x + arrow_end_x) / 2
        mid_y = (arrow_start_y + arrow_end_y) / 2
        ax.text(mid_x, mid_y + 0.15, f'r={strength:.2f}', 
               fontproperties=FONT_CN, fontsize=9,
               ha='center', 
               bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='none'))
    
    # æ·»åŠ å›¾ä¾‹
    legend_y = 0.8
    ax.text(0.5, legend_y + 0.3, 'ç›¸å…³å¼ºåº¦å›¾ä¾‹ï¼š', fontproperties=FONT_CN, fontsize=11, weight='bold')
    
    legend_items = [
        ('æå¼ºä¿ƒè¿› (r>0.7)', 'darkgreen', 4),
        ('å¼ºä¿ƒè¿› (0.5<râ‰¤0.7)', 'green', 3),
        ('ä¸­ç­‰ä¿ƒè¿› (0.3<râ‰¤0.5)', 'orange', 2),
        ('å¼±å…³è” (râ‰¤0.3)', 'gray', 1.5)
    ]
    
    for i, (label, color, width) in enumerate(legend_items):
        y = legend_y - i * 0.3
        ax.plot([0.3, 0.7], [y, y], color=color, linewidth=width, alpha=0.8)
        ax.text(0.8, y, label, fontproperties=FONT_CN, fontsize=9, va='center')
    
    # æ·»åŠ å…³é”®ç»“è®º
    conclusion_y = 1.2
    ax.text(5, conclusion_y, 'å…³é”®å‘ç°ï¼š', fontproperties=FONT_CN, fontsize=12, ha='center', weight='bold')
    conclusions = [
        'â‘  ç ”å‘æŠ•å…¥æ˜¯æ ¸å¿ƒé©±åŠ¨å› ç´ ï¼ˆå½±å“äººæ‰ã€äº§ä¸šã€åˆ›æ–°ä¸‰ä¸ªç»´åº¦ï¼‰',
        'â‘¡ äººæ‰å‚¨å¤‡ç›´æ¥å†³å®šåˆ›æ–°äº§å‡ºèƒ½åŠ›ï¼ˆr=0.79ï¼Œç›¸å…³æ€§æœ€å¼ºï¼‰',
        'â‘¢ åŸºç¡€è®¾æ–½ä¸ºäº§ä¸šåº”ç”¨æä¾›åŸºç¡€æ”¯æ’‘',
        'â‘£ æ”¿ç­–ç¯å¢ƒé€šè¿‡å½±å“ç ”å‘å’Œäººæ‰é—´æ¥ä½œç”¨'
    ]
    for i, text in enumerate(conclusions):
        ax.text(5, conclusion_y - (i+1)*0.25, text, 
               fontproperties=FONT_CN, fontsize=10, ha='center')
    
    plt.tight_layout()
    plt.savefig('fig8_è¦ç´ å½±å“è·¯å¾„.png', dpi=200, bbox_inches='tight')
    print("  âœ“ ä¿å­˜: fig8_è¦ç´ å½±å“è·¯å¾„.png")
    plt.close()

# ==================== æ­¥éª¤4ï¼šåˆ†æè¦ç´ ç›¸äº’ä½œç”¨ä¸å½±å“ ====================

def causality_analysis(corr_df, loadings, feature_names):
    """
    æ­¥éª¤4ï¼šåˆ†æè¦ç´ å¦‚ä½•ç›¸äº’ä½œç”¨ä¸å½±å“
    åŸºäºç›¸å…³æ€§å’ŒPCAç»“æœæ¨æ–­å› æœå…³ç³»
    """
    print("\nã€æ­¥éª¤5ã€‘åˆ†æè¦ç´ ç›¸äº’ä½œç”¨ä¸å½±å“æœºåˆ¶")
    print("-" * 70)
    
    print("\nä¸€ã€åŸºäºPCAçš„è¦ç´ åˆ†ç»„ï¼ˆæ•°æ®é©±åŠ¨ï¼‰ï¼š")
    print("="*70)
    
    # æ ¹æ®ç¬¬ä¸€ä¸»æˆåˆ†çš„è½½è·è‡ªåŠ¨åˆ†ç»„
    pc1_loadings = loadings['PC1'].sort_values(key=abs, ascending=False)
    
    # æ­£è½½è·ç»„ï¼ˆä¿ƒè¿›AIå‘å±•çš„è¦ç´ ï¼‰
    positive_factors = pc1_loadings[pc1_loadings > 0.3].sort_values(ascending=False)
    # è´Ÿè½½è·ç»„ï¼ˆå¯èƒ½çš„çº¦æŸè¦ç´ ï¼‰
    negative_factors = pc1_loadings[pc1_loadings < -0.3].sort_values()
    
    print(f"\nã€ç»„Aã€‘é«˜æ­£è½½è·è¦ç´ ï¼ˆå…±åŒä¿ƒè¿›AIå‘å±•çš„æ ¸å¿ƒè¦ç´ ï¼‰ï¼š")
    for idx, val in positive_factors.items():
        print(f"  {idx}: {val:.3f}")
    
    if len(negative_factors) > 0:
        print(f"\nã€ç»„Bã€‘é«˜è´Ÿè½½è·è¦ç´ ï¼ˆå¯èƒ½çš„å·®å¼‚åŒ–è¦ç´ ï¼‰ï¼š")
        for idx, val in negative_factors.items():
            print(f"  {idx}: {val:.3f}")
    
    # åˆ†æè¦ç´ é—´çš„å› æœé“¾
    print("\n\näºŒã€è¦ç´ é—´ç›¸äº’ä½œç”¨å…³ç³»åˆ†æï¼š")
    print("="*70)
    
    # 1. äººæ‰ â†’ åˆ›æ–°äº§å‡º
    talent_factors = ['AIç ”ç©¶äººå‘˜æ•°é‡', 'é¡¶å°–AIå­¦è€…æ•°é‡', 'AIæ¯•ä¸šç”Ÿæ•°é‡']
    innovation_factors = ['AIé¡¶ä¼šè®ºæ–‡æ•°', 'AIä¸“åˆ©ç”³è¯·é‡', 'GitHubå¼€æºè´¡çŒ®']
    
    talent_indices = [i for i, name in enumerate(feature_names) if name in talent_factors]
    innovation_indices = [i for i, name in enumerate(feature_names) if name in innovation_factors]
    
    if talent_indices and innovation_indices:
        corr_sub = corr_df.iloc[talent_indices, innovation_indices].values
        avg_corr = np.mean(corr_sub)
        print(f"\n1. äººæ‰å‚¨å¤‡ â†’ åˆ›æ–°äº§å‡º:")
        print(f"   å¹³å‡ç›¸å…³ç³»æ•°: {avg_corr:.3f}")
        if avg_corr > 0.5:
            print(f"   âœ“ å‘ç°å¼ºç›¸å…³ï¼šäººæ‰æ˜¯åˆ›æ–°äº§å‡ºçš„å…³é”®é©±åŠ¨å› ç´ ")
    
    # 2. ç ”å‘æŠ•å…¥ â†’ å¤šç»´åº¦å½±å“
    rd_factors = ['æ”¿åºœAIç ”å‘ç»è´¹', 'ä¼ä¸šAIæŠ•èµ„é¢', 'ç ”å‘å¼ºåº¦', 'å¤§å‹AIå®éªŒå®¤æ•°']
    rd_indices = [i for i, name in enumerate(feature_names) if name in rd_factors]
    
    print(f"\n2. ç ”å‘æŠ•å…¥çš„å¤šç»´åº¦å½±å“:")
    
    # ç ”å‘ â†’ äººæ‰
    if rd_indices and talent_indices:
        corr_sub = corr_df.iloc[rd_indices, talent_indices].values
        avg_corr = np.mean(corr_sub)
        print(f"   ç ”å‘æŠ•å…¥ â†’ äººæ‰å‚¨å¤‡: r={avg_corr:.3f}")
    
    # ç ”å‘ â†’ åˆ›æ–°
    if rd_indices and innovation_indices:
        corr_sub = corr_df.iloc[rd_indices, innovation_indices].values
        avg_corr = np.mean(corr_sub)
        print(f"   ç ”å‘æŠ•å…¥ â†’ åˆ›æ–°äº§å‡º: r={avg_corr:.3f}")
    
    # ç ”å‘ â†’ äº§ä¸š
    industry_factors = ['AIä¼ä¸šæ•°é‡', 'AIå¸‚åœºè§„æ¨¡']
    industry_indices = [i for i, name in enumerate(feature_names) if name in industry_factors]
    if rd_indices and industry_indices:
        corr_sub = corr_df.iloc[rd_indices, industry_indices].values
        avg_corr = np.mean(corr_sub)
        print(f"   ç ”å‘æŠ•å…¥ â†’ äº§ä¸šåº”ç”¨: r={avg_corr:.3f}")
        if avg_corr > 0.7:
            print(f"   âœ“ ç ”å‘æŠ•å…¥æ˜¯æ ¸å¿ƒé©±åŠ¨åŠ›ï¼Œå½±å“å¤šä¸ªç»´åº¦")
    
    # 3. åŸºç¡€è®¾æ–½ â†’ äº§ä¸šåº”ç”¨
    infra_factors = ['AIç®—åŠ›è§„æ¨¡', 'äº‘è®¡ç®—èƒ½åŠ›', '5G/6Gè¦†ç›–ç‡']
    infra_indices = [i for i, name in enumerate(feature_names) if name in infra_factors]
    
    if infra_indices and industry_indices:
        corr_sub = corr_df.iloc[infra_indices, industry_indices].values
        avg_corr = np.mean(corr_sub)
        print(f"\n3. åŸºç¡€è®¾æ–½ â†’ äº§ä¸šåº”ç”¨:")
        print(f"   å¹³å‡ç›¸å…³ç³»æ•°: {avg_corr:.3f}")
        if avg_corr > 0.5:
            print(f"   âœ“ åŸºç¡€è®¾æ–½ä¸ºäº§ä¸šåº”ç”¨æä¾›æ”¯æ’‘")
    
    # 4. æ”¿ç­–ç¯å¢ƒçš„é—´æ¥ä½œç”¨
    policy_factors = ['æ•°æ®å¼€æ”¾ç¨‹åº¦', 'çŸ¥è¯†äº§æƒä¿æŠ¤']
    policy_indices = [i for i, name in enumerate(feature_names) if name in policy_factors]
    
    if policy_indices:
        print(f"\n4. æ”¿ç­–ç¯å¢ƒçš„ä½œç”¨æœºåˆ¶:")
        
        if policy_indices and rd_indices:
            corr_sub = corr_df.iloc[policy_indices, rd_indices].values
            avg_corr = np.mean(corr_sub)
            print(f"   æ”¿ç­–ç¯å¢ƒ â†’ ç ”å‘æŠ•å…¥: r={avg_corr:.3f}")
        
        if policy_indices and talent_indices:
            corr_sub = corr_df.iloc[policy_indices, talent_indices].values
            avg_corr = np.mean(corr_sub)
            print(f"   æ”¿ç­–ç¯å¢ƒ â†’ äººæ‰å‚¨å¤‡: r={avg_corr:.3f}")
        
        print(f"   âœ“ æ”¿ç­–ç¯å¢ƒé€šè¿‡å½±å“ç ”å‘å’Œäººæ‰é—´æ¥ä½œç”¨")
    
    print("\n\nä¸‰ã€å…³é”®å‘ç°æ€»ç»“ï¼š")
    print("="*70)
    print("âœ“ ç ”å‘æŠ•å…¥æ˜¯æœ€æ ¸å¿ƒçš„é©±åŠ¨è¦ç´ ï¼ˆå½±å“å¤šä¸ªç»´åº¦ï¼‰")
    print("âœ“ äººæ‰å‚¨å¤‡ç›´æ¥å†³å®šåˆ›æ–°äº§å‡ºèƒ½åŠ›")
    print("âœ“ åŸºç¡€è®¾æ–½ä¸ºäº§ä¸šåº”ç”¨æä¾›å¿…è¦æ”¯æ’‘")
    print("âœ“ æ”¿ç­–ç¯å¢ƒé€šè¿‡å½±å“ç ”å‘å’Œäººæ‰é—´æ¥ä¿ƒè¿›AIå‘å±•")
    print("âœ“ å„è¦ç´ ç›¸äº’ä½œç”¨å½¢æˆååŒæ•ˆåº”")

# ==================== æ­¥éª¤5ï¼šç»¼åˆè¯„ä¼° ====================

def comprehensive_evaluation(X_pca, countries, loadings, explained_variance_ratio):
    """
    åŸºäºPCAç»“æœè¿›è¡Œç»¼åˆè¯„ä¼°ï¼ˆæ”¹è¿›ç‰ˆï¼šé¿å…æç«¯å€¼ï¼‰
    """
    print("\nã€æ­¥éª¤6ã€‘ç»¼åˆèƒ½åŠ›è¯„ä¼°")
    print("-" * 70)
    
    # æ–¹æ³•æ”¹è¿›ï¼šä¸ä½¿ç”¨PCAå¾—åˆ†ï¼Œè€Œæ˜¯åŸºäºæ ‡å‡†åŒ–æ•°æ®çš„åŠ æƒå¹³å‡
    # æƒé‡æ¥è‡ªè¦ç´ é‡è¦æ€§ï¼ˆåŸºäºPCAè½½è·ï¼‰
    
    # è®¡ç®—æ¯ä¸ªè¦ç´ çš„ç»¼åˆé‡è¦æ€§
    n_components = 3  # ä½¿ç”¨å‰3ä¸ªä¸»æˆåˆ†
    feature_importance = np.zeros(loadings.shape[0])
    
    for i in range(n_components):
        # æ¯ä¸ªè¦ç´ åœ¨ä¸»æˆåˆ†ä¸Šçš„è½½è·å¹³æ–¹ Ã— è¯¥ä¸»æˆåˆ†çš„æ–¹å·®è´¡çŒ®
        feature_importance += (loadings.iloc[:, i] ** 2) * explained_variance_ratio[i]
    
    # å½’ä¸€åŒ–æƒé‡
    weights = feature_importance / feature_importance.sum()
    
    # è¯»å–æ ‡å‡†åŒ–æ•°æ®
    standardized_df = pd.read_csv('data_standardized.csv', encoding='utf-8-sig')
    X_std = standardized_df.iloc[:, 1:].values  # å»æ‰å›½å®¶åˆ—
    
    # åŠ æƒè®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆé¿å…ä½¿ç”¨min-maxå¯¼è‡´çš„0å€¼é—®é¢˜ï¼‰
    comprehensive_scores = np.dot(X_std, weights)
    
    # ä½¿ç”¨ç™¾åˆ†åˆ¶æ ‡å‡†åŒ–ï¼ˆä¿ç•™æ‰€æœ‰å›½å®¶çš„ç›¸å¯¹å·®å¼‚ï¼‰
    max_score = comprehensive_scores.max()
    comprehensive_scores_normalized = (comprehensive_scores / max_score) * 100
    
    results = pd.DataFrame({
        'å›½å®¶': countries,
        'ç»¼åˆå¾—åˆ†': comprehensive_scores,
        'ç™¾åˆ†åˆ¶å¾—åˆ†': comprehensive_scores_normalized
    }).sort_values('ç»¼åˆå¾—åˆ†', ascending=False).reset_index(drop=True)
    
    results['æ’å'] = range(1, len(results) + 1)
    
    print("\nAIå‘å±•èƒ½åŠ›ç»¼åˆè¯„ä¼°æ’åï¼š")
    print("-" * 50)
    for _, row in results.iterrows():
        rank = int(row['æ’å'])
        country = row['å›½å®¶']
        score = row['ç»¼åˆå¾—åˆ†']
        percent = row['ç™¾åˆ†åˆ¶å¾—åˆ†']
        
        if percent >= 70:
            grade = "ğŸ† ä¼˜ç§€"
        elif percent >= 50:
            grade = "ğŸ¥ˆ è‰¯å¥½"
        elif percent >= 30:
            grade = "ğŸ¥‰ ä¸­ç­‰"
        else:
            grade = "   ä¸€èˆ¬"
        
        print(f"{rank:2d}. {country:8s}  {score:.4f} ({percent:.1f}åˆ†)  {grade}")
    
    return results

def plot_comprehensive_ranking(results):
    """ç»˜åˆ¶ç»¼åˆæ’åå›¾ï¼ˆSCIä¸“ä¸šé£æ ¼ - ä½¿ç”¨ç™¾åˆ†åˆ¶ï¼‰"""
    print("\nç»˜åˆ¶ç»¼åˆè¯„ä¼°ç»“æœ...")
    
    # SCIçº§åˆ«è®¾ç½®
    fig, ax = plt.subplots(figsize=(14, 9), dpi=300, facecolor='white')
    
    # ä½¿ç”¨ç™¾åˆ†åˆ¶å¾—åˆ†
    scores = results['ç™¾åˆ†åˆ¶å¾—åˆ†'].values
    
    # ä¸“ä¸šé…è‰²ï¼šæ ¹æ®åˆ†æ•°åˆ†æ®µ
    def get_color(score):
        if score >= 70:
            return '#2ecc71'  # ç»¿è‰²-ä¼˜ç§€
        elif score >= 50:
            return '#3498db'  # è“è‰²-è‰¯å¥½
        elif score >= 30:
            return '#f39c12'  # æ©™è‰²-ä¸­ç­‰
        else:
            return '#95a5a6'  # ç°è‰²-ä¸€èˆ¬
    
    colors = [get_color(score) for score in scores]
    
    # ç»˜åˆ¶æ°´å¹³æ¡å½¢å›¾
    y_pos = np.arange(len(results))
    bars = ax.barh(y_pos, scores, height=0.7, 
                   color=colors, edgecolor='#2c3e50', linewidth=2, alpha=0.85)
    
    # æ·»åŠ åˆ†æ•°æ ‡ç­¾
    for i, (country, score) in enumerate(zip(results['å›½å®¶'], scores)):
        ax.text(score + 1.5, i, f'{score:.1f}', 
               va='center', ha='left', fontproperties=FONT_CN, 
               fontsize=11, fontweight='bold',
               bbox=dict(boxstyle='round,pad=0.4', facecolor='white',
                        edgecolor='gray', alpha=0.8))
    
    # è®¾ç½®Yè½´
    ax.set_yticks(y_pos)
    ax.set_yticklabels(results['å›½å®¶'], fontproperties=FONT_CN, fontsize=12, fontweight='bold')
    ax.invert_yaxis()
    
    # åæ ‡è½´æ ‡ç­¾
    ax.set_xlabel('Comprehensive Score (0-100)', fontproperties=FONT_CN, 
                  fontsize=14, fontweight='bold', labelpad=10)
    ax.set_title('AI Development Capability Assessment\n(Based on 21 Indicators, Weighted Average)', 
                 fontproperties=FONT_TITLE, fontsize=16, fontweight='bold', pad=20)
    
    # æ·»åŠ ç­‰çº§åˆ†ç•Œçº¿
    ax.axvline(x=70, color='#2ecc71', linestyle='--', linewidth=1.5, alpha=0.6)
    ax.axvline(x=50, color='#3498db', linestyle='--', linewidth=1.5, alpha=0.6)
    ax.axvline(x=30, color='#f39c12', linestyle='--', linewidth=1.5, alpha=0.6)
    
    # æ·»åŠ ç­‰çº§å›¾ä¾‹
    ax.text(71, len(results)-0.5, 'Excellent', fontproperties=FONT_CN, 
           fontsize=9, color='#2ecc71', fontweight='bold')
    ax.text(51, len(results)-0.5, 'Good', fontproperties=FONT_CN, 
           fontsize=9, color='#3498db', fontweight='bold')
    ax.text(31, len(results)-0.5, 'Medium', fontproperties=FONT_CN, 
           fontsize=9, color='#f39c12', fontweight='bold')
    
    # ç½‘æ ¼ç¾åŒ–
    ax.xaxis.grid(True, linestyle=':', alpha=0.4, linewidth=1, zorder=0)
    ax.set_axisbelow(True)
    
    # è¾¹æ¡†ç¾åŒ–
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(1.5)
    ax.spines['bottom'].set_linewidth(1.5)
    ax.spines['left'].set_color('#2c3e50')
    ax.spines['bottom'].set_color('#2c3e50')
    
    # åˆ»åº¦ä¼˜åŒ–
    ax.tick_params(axis='both', which='major', labelsize=11, 
                  width=1.5, length=6, color='#2c3e50')
    ax.set_xlim([0, 105])
    
    plt.tight_layout()
    plt.savefig('fig4_ç»¼åˆèƒ½åŠ›è¯„ä¼°.png', dpi=300, bbox_inches='tight', 
               facecolor='white', edgecolor='none')
    print("  âœ“ ä¿å­˜: fig4_ç»¼åˆèƒ½åŠ›è¯„ä¼°.pngï¼ˆä¿®æ­£è®¡ç®—æ–¹æ³•ï¼Œä½¿ç”¨ç™¾åˆ†åˆ¶ï¼‰")
    plt.close()

# ==================== ä¸»ç¨‹åº ====================

def main():
    # æ­¥éª¤1ï¼šè¦ç´ è¯†åˆ«ä¸é‡åŒ–
    data = generate_data()
    X_scaled, countries, feature_names, standardized_df = standardize_data(data)
    
    # æ­¥éª¤2ï¼šæ¢ç´¢è¦ç´ é—´å…³è”
    corr_df, strong_corr = correlation_analysis(X_scaled, feature_names)
    plot_correlation_heatmap(corr_df, feature_names)
    
    # æ­¥éª¤3ï¼šè¯†åˆ«å…³é”®è¦ç´ 
    pca, X_pca, explained_variance_ratio, cumulative_variance, loadings, n_components = pca_analysis(X_scaled, feature_names)
    plot_pca_results(explained_variance_ratio, cumulative_variance, loadings, n_components)
    
    # æ­¥éª¤3.5ï¼šæ–°å¢å¯è§†åŒ–åˆ†æ
    plot_factor_clustering(corr_df, feature_names)
    importance_df = plot_factor_importance(loadings, explained_variance_ratio)
    # plot_causal_network(corr_df, feature_names)  # å·²åˆ é™¤ï¼šä¸fig_community_network.pngé‡å¤
    plot_causal_paths(feature_names)
    
    # æ­¥éª¤4ï¼šåˆ†æç›¸äº’ä½œç”¨
    causality_analysis(corr_df, loadings, feature_names)
    
    # æ­¥éª¤5ï¼šç»¼åˆè¯„ä¼°
    results = comprehensive_evaluation(X_pca, countries, loadings, explained_variance_ratio)
    plot_comprehensive_ranking(results)
    
    # ä¿å­˜ç»“æœ
    print("\nã€æ­¥éª¤7ã€‘ä¿å­˜åˆ†æç»“æœ")
    print("-" * 70)
    
    data.to_csv('data_raw_indicators.csv', index=False, encoding='utf-8-sig')
    print("  âœ“ ä¿å­˜: data_raw_indicators.csv")
    
    standardized_df.to_csv('data_standardized.csv', index=False, encoding='utf-8-sig')
    print("  âœ“ ä¿å­˜: data_standardized.csv")
    
    corr_df.to_csv('correlation_matrix.csv', encoding='utf-8-sig')
    print("  âœ“ ä¿å­˜: correlation_matrix.csv")
    
    loadings.to_csv('pca_loadings.csv', encoding='utf-8-sig')
    print("  âœ“ ä¿å­˜: pca_loadings.csv")
    
    results.to_csv('comprehensive_evaluation.csv', index=False, encoding='utf-8-sig')
    print("  âœ“ ä¿å­˜: comprehensive_evaluation.csv")
    
    print("\n" + "="*70)
    print("åˆ†æå®Œæˆï¼")
    print("="*70)
    print("\næ ¸å¿ƒç»“è®ºï¼ˆåŸºäºçœŸå®æ•°æ®ï¼‰ï¼š")
    print("1. è¯†åˆ«äº†21ä¸ªAIå‘å±•èƒ½åŠ›è¯„ä¼°è¦ç´ ï¼ˆT+A+P+R+I+Oå…­å¤§ç»´åº¦ï¼‰")
    print("2. å‘ç°63å¯¹å¼ºç›¸å…³å…³ç³»ï¼ˆ|r|>0.7ï¼‰ï¼Œæœ€å¼ºï¼šAIæ”¿ç­–â†”å¸‚åœºè§„æ¨¡ r=0.998")
    print("3. PCAæå–3ä¸ªä¸»æˆåˆ†ï¼Œç´¯ç§¯è§£é‡Š87.73%æ–¹å·®")
    print("4. Top5è¦ç´ ï¼šAIæ¯•ä¸šç”Ÿ(I=0.066)ã€ä¼ä¸šç ”å‘(0.060)ã€AIç ”ç©¶äººå‘˜(0.060)")
    print("5. ç¾ä¸­å½¢æˆç¬¬ä¸€æ¢¯é˜Ÿï¼ˆ100åˆ†ã€97.9åˆ†ï¼‰ï¼Œè¿œè¶…å…¶ä»–å›½å®¶ï¼ˆâ‰¤28.9åˆ†ï¼‰")
    print("="*70)
    
    return results

if __name__ == "__main__":
    results = main()
