\section{Task 3: AI Competitiveness Ranking Forecast (2026--2035)}

\subsection{Model Objective and Overall Framework}

Building upon the indicator system and structural insights identified in Task~1, as well as the objective evaluation framework established in Task~2, Task~3 focused on forecasting the evolution of national AI competitiveness rankings over the period 2026--2035. The primary objective was to construct a \textit{forecast--evaluation integrated pipeline} that remained fully reproducible, interpretable, and strictly consistent with the previously defined evaluation model.

Let $x_{i,j,t}$ denote the observed value of indicator $j$ for country $i$ in year $t$. Using the complete panel data from 2016 to 2025, Task~3 was implemented through three sequential stages:
\begin{enumerate}
\item indicator-level time series forecasting for each country and indicator;
\item annual comprehensive evaluation using the fixed weighting and TOPSIS framework from Task~2;
\item ranking evolution and driver analysis across the forecast horizon.
\end{enumerate}

This design ensured that all ranking changes were driven exclusively by data-inferred temporal dynamics, without introducing any exogenous assumptions or subjective adjustments.

\subsection{Data Structure and Notation}

The data used in Task~3 followed a balanced panel structure:
\begin{itemize}
\item Countries: $i = 1, \dots, n$, with $n=10$;
\item Indicators: $j = 1, \dots, p$, with $p=24$;
\item Historical years: $t \in \{2016, \dots, 2025\}$;
\item Forecast years: $t \in \{2026, \dots, 2035\}$.
\end{itemize}

For each forecast year $t$, the predicted indicator matrix was denoted as
\begin{equation}
\hat{X}_{t} = \bigl(\hat{x}_{i,j,t}\bigr)_{n \times p},
\end{equation}
which served as the direct input to the annual evaluation procedure.

To preserve physical interpretability and comparability across indicators, indicator-specific constraints were imposed:
\begin{itemize}
\item non-negativity constraints for scale and quantity indicators;
\item bounded intervals for ratio- or proportion-type indicators.
\end{itemize}

\subsection{Indicator Forecasting Methodology}

\subsubsection{Baseline Grey Forecasting Model}

Given the limited historical sample size available for each indicator sequence, the Grey Model GM(1,1) was adopted as the baseline forecasting approach. For a fixed country--indicator pair $(i,j)$, the original sequence was defined as
\begin{equation}
X^{(0)} = \bigl(x^{(0)}(1), x^{(0)}(2), \dots, x^{(0)}(T)\bigr), \quad T=10.
\end{equation}

After applying first-order accumulated generation (AGO), a first-order differential equation was fitted and solved to obtain the corresponding time response function. The predicted original sequence was then recovered via inverse AGO. The GM(1,1) model was selected for its robustness in small-sample settings and its suitability for capturing monotonic development trends.

\subsubsection{Engineering Constraints and Stabilization}

To ensure numerical stability and realistic forecasts across heterogeneous indicators, several unified treatments were applied:
\begin{itemize}
\item non-negative translation for near-zero or zero-valued series prior to model fitting;
\item boundary truncation of forecast values according to indicator type;
\item optional logarithmic transformation for indicators with large magnitude dispersion.
\end{itemize}

All transformations were rule-based and reversible, and no country-specific adjustments were introduced at any stage.

\subsubsection{Backtesting and Fallback Strategy}

Model suitability was evaluated via one-step-ahead backtesting. Specifically, data from 2016--2024 were used for training, and the year 2025 was forecast for validation. Prediction error was measured using the mean absolute percentage error (MAPE).

\textit{Result placeholder.}\\
\textbf{To be added:} summary statistics of backtesting errors across all country--indicator pairs, using data from \texttt{forecast\_diagnostics.csv}.\\
\textbf{Suggested table:} Three-line table reporting the distribution of MAPE values and the proportion of series exceeding the predefined threshold.

When the GM(1,1) model failed to satisfy accuracy or stability requirements, a fallback predictor (linear trend or Holt exponential smoothing) was automatically activated, subject to the same boundary constraints.

\subsection{Annual Evaluation Using Fixed Weights and TOPSIS}

\subsubsection{Weight Inheritance Principle}

Let $W = (w_1, \dots, w_p)$ denote the entropy-based indicator weights obtained in Task~2. Throughout Task~3, these weights were held constant across all forecast years to ensure temporal comparability and methodological consistency.

\subsubsection{Yearly TOPSIS Evaluation}

For each forecast year $t$, the predicted matrix $\hat{X}_{t}$ was normalized and weighted using the same TOPSIS procedure as in Task~2. The relative closeness score for country $i$ in year $t$ was denoted as
\begin{equation}
C_{i,t} \in [0,1].
\end{equation}

\textit{Result placeholder.}\\
\textbf{To be added:} annual TOPSIS scores for all countries during 2026--2035, based on \texttt{topsis\_scores\_2026\_2035.csv}.\\
\textbf{Suggested table:} Three-line table showing selected years (e.g., 2026, 2030, 2035) with corresponding scores for each country.\\
\textbf{Suggested figure:} Line plot illustrating the temporal evolution of $C_{i,t}$ for major economies.

\subsection{Ranking Generation and Evolution Analysis}

For each year $t$, countries were ranked in descending order of $C_{i,t}$:
\begin{equation}
\mathrm{Rank}_{i,t} = \mathrm{rankdesc}\!\left(C_{:,t}\right).
\end{equation}

\textit{Result placeholder.}\\
\textbf{To be added:} complete ranking results for 2026--2035 using \texttt{rankings\_2026\_2035.csv}.\\
\textbf{Suggested table:} Three-line table summarizing average rank, maximum rise/fall, and rank standard deviation for each country.\\
\textbf{Suggested figure:} Ranking evolution plot or heatmap based on \texttt{rankings\_2026\_2035\_wide.csv}.

\subsection{Driving Factor Analysis of Ranking Changes}

To interpret ranking dynamics, changes in the TOPSIS score were decomposed with respect to high-weight indicators. For a fixed country, score variations were analyzed in conjunction with changes in weighted normalized indicator values.

\textit{Result placeholder.}\\
\textbf{To be added:} identification of key indicators contributing to score changes for representative countries.\\
\textbf{Data source:} weighted normalized matrices derived from predicted indicators and Task~2 weights.\\
\textbf{Suggested figure:} Contribution bar charts or radar plots highlighting dominant drivers.

\subsection{Model Validation and Robustness Checks}

Model robustness was examined from both the forecasting and evaluation perspectives:
\begin{itemize}
\item comparison between predicted and observed 2025 evaluation scores to validate the end-to-end pipeline;
\item sensitivity of rankings to fallback thresholds and minor perturbations in high-weight indicators.
\end{itemize}

\textit{Result placeholder.}\\
\textbf{To be added:} robustness assessment results using sensitivity scenarios.\\
\textbf{Suggested table:} Three-line table reporting rank correlations under different perturbation settings.

\subsection{Summary of Task 3}

Task~3 extended the static evaluation framework into a dynamic forecasting context by integrating indicator-level time series prediction with a fixed and validated evaluation model. The resulting rankings reflected endogenous development trajectories inferred from historical data, rather than externally imposed assumptions.

Overall, the proposed pipeline preserved methodological continuity across tasks, maintained interpretability at both the indicator and system levels, and provided a quantitative basis for analyzing future shifts in global AI competitiveness. These results established a consistent foundation for the subsequent policy-oriented optimization analysis.
